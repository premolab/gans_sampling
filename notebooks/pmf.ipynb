{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "api_path_ml_est = os.path.join(cwd, '..', 'tools', 'ml_estimation')\n",
    "api_path_data = os.path.join(api_path_ml_est, 'data', 'ml-100k', 'u.data')\n",
    "sys.path.append(api_path_ml_est)\n",
    "sys.path.append(os.path.join(cwd, '..', 'tools', 'sampling_utils'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from LoadData_pmf import load_rating_data, spilt_rating_dat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pmf import PMF, PMF_torch\n",
    "import random\n",
    "import torch, torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from sir_ais_sampling import (sir_correlated_dynamics,\n",
    "                              sir_independent_dynamics)\n",
    "\n",
    "from distributions import (Target, \n",
    "                           Gaussian_mixture, \n",
    "                           IndependentNormal,\n",
    "                           init_independent_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 1682 10\n"
     ]
    }
   ],
   "source": [
    "pmf = PMF()\n",
    "pmf.set_params({\"num_feat\": 10, \n",
    "                \"epsilon\": 10, \n",
    "                \"_lambda\": 0.1, \n",
    "                \"momentum\": 0.8, \n",
    "                \"maxepoch\": 300, \n",
    "                \"num_batches\": 1,\n",
    "                \"batch_size\": 100000})\n",
    "ratings = load_rating_data(api_path_data)\n",
    "print(len(np.unique(ratings[:, 0])), len(np.unique(ratings[:, 1])), pmf.num_feat)\n",
    "train, test = train_test_split(ratings, test_size=0.2)  # spilt_rating_dat(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 42\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training RMSE: 1.127441, Test RMSE 1.121133\n",
      "Epoch: 2, Training RMSE: 1.127338, Test RMSE 1.121123\n",
      "Epoch: 3, Training RMSE: 1.127201, Test RMSE 1.121109\n",
      "Epoch: 4, Training RMSE: 1.127037, Test RMSE 1.121093\n",
      "Epoch: 5, Training RMSE: 1.126855, Test RMSE 1.121076\n",
      "Epoch: 6, Training RMSE: 1.126660, Test RMSE 1.121057\n",
      "Epoch: 7, Training RMSE: 1.126456, Test RMSE 1.121037\n",
      "Epoch: 8, Training RMSE: 1.126247, Test RMSE 1.121017\n",
      "Epoch: 9, Training RMSE: 1.126035, Test RMSE 1.120996\n",
      "Epoch: 10, Training RMSE: 1.125821, Test RMSE 1.120974\n",
      "Epoch: 11, Training RMSE: 1.125608, Test RMSE 1.120952\n",
      "Epoch: 12, Training RMSE: 1.125395, Test RMSE 1.120929\n",
      "Epoch: 13, Training RMSE: 1.125182, Test RMSE 1.120904\n",
      "Epoch: 14, Training RMSE: 1.124971, Test RMSE 1.120879\n",
      "Epoch: 15, Training RMSE: 1.124760, Test RMSE 1.120851\n",
      "Epoch: 16, Training RMSE: 1.124549, Test RMSE 1.120822\n",
      "Epoch: 17, Training RMSE: 1.124337, Test RMSE 1.120790\n",
      "Epoch: 18, Training RMSE: 1.124125, Test RMSE 1.120756\n",
      "Epoch: 19, Training RMSE: 1.123910, Test RMSE 1.120720\n",
      "Epoch: 20, Training RMSE: 1.123693, Test RMSE 1.120679\n",
      "Epoch: 21, Training RMSE: 1.123473, Test RMSE 1.120633\n",
      "Epoch: 22, Training RMSE: 1.123248, Test RMSE 1.120584\n",
      "Epoch: 23, Training RMSE: 1.123018, Test RMSE 1.120528\n",
      "Epoch: 24, Training RMSE: 1.122780, Test RMSE 1.120466\n",
      "Epoch: 25, Training RMSE: 1.122535, Test RMSE 1.120397\n",
      "Epoch: 26, Training RMSE: 1.122280, Test RMSE 1.120319\n",
      "Epoch: 27, Training RMSE: 1.122015, Test RMSE 1.120233\n",
      "Epoch: 28, Training RMSE: 1.121737, Test RMSE 1.120137\n",
      "Epoch: 29, Training RMSE: 1.121445, Test RMSE 1.120030\n",
      "Epoch: 30, Training RMSE: 1.121138, Test RMSE 1.119911\n",
      "Epoch: 31, Training RMSE: 1.120813, Test RMSE 1.119778\n",
      "Epoch: 32, Training RMSE: 1.120468, Test RMSE 1.119630\n",
      "Epoch: 33, Training RMSE: 1.120102, Test RMSE 1.119463\n",
      "Epoch: 34, Training RMSE: 1.119710, Test RMSE 1.119278\n",
      "Epoch: 35, Training RMSE: 1.119291, Test RMSE 1.119070\n",
      "Epoch: 36, Training RMSE: 1.118841, Test RMSE 1.118838\n",
      "Epoch: 37, Training RMSE: 1.118358, Test RMSE 1.118580\n",
      "Epoch: 38, Training RMSE: 1.117839, Test RMSE 1.118292\n",
      "Epoch: 39, Training RMSE: 1.117278, Test RMSE 1.117969\n",
      "Epoch: 40, Training RMSE: 1.116671, Test RMSE 1.117611\n",
      "Epoch: 41, Training RMSE: 1.116015, Test RMSE 1.117212\n",
      "Epoch: 42, Training RMSE: 1.115303, Test RMSE 1.116768\n",
      "Epoch: 43, Training RMSE: 1.114528, Test RMSE 1.116274\n",
      "Epoch: 44, Training RMSE: 1.113686, Test RMSE 1.115725\n",
      "Epoch: 45, Training RMSE: 1.112773, Test RMSE 1.115118\n",
      "Epoch: 46, Training RMSE: 1.111780, Test RMSE 1.114444\n",
      "Epoch: 47, Training RMSE: 1.110697, Test RMSE 1.113697\n",
      "Epoch: 48, Training RMSE: 1.109520, Test RMSE 1.112872\n",
      "Epoch: 49, Training RMSE: 1.108240, Test RMSE 1.111960\n",
      "Epoch: 50, Training RMSE: 1.106846, Test RMSE 1.110956\n",
      "Epoch: 51, Training RMSE: 1.105331, Test RMSE 1.109849\n",
      "Epoch: 52, Training RMSE: 1.103687, Test RMSE 1.108635\n",
      "Epoch: 53, Training RMSE: 1.101904, Test RMSE 1.107304\n",
      "Epoch: 54, Training RMSE: 1.099972, Test RMSE 1.105847\n",
      "Epoch: 55, Training RMSE: 1.097885, Test RMSE 1.104260\n",
      "Epoch: 56, Training RMSE: 1.095638, Test RMSE 1.102540\n",
      "Epoch: 57, Training RMSE: 1.093223, Test RMSE 1.100678\n",
      "Epoch: 58, Training RMSE: 1.090633, Test RMSE 1.098671\n",
      "Epoch: 59, Training RMSE: 1.087864, Test RMSE 1.096515\n",
      "Epoch: 60, Training RMSE: 1.084919, Test RMSE 1.094217\n",
      "Epoch: 61, Training RMSE: 1.081797, Test RMSE 1.091774\n",
      "Epoch: 62, Training RMSE: 1.078502, Test RMSE 1.089191\n",
      "Epoch: 63, Training RMSE: 1.075036, Test RMSE 1.086473\n",
      "Epoch: 64, Training RMSE: 1.071410, Test RMSE 1.083627\n",
      "Epoch: 65, Training RMSE: 1.067637, Test RMSE 1.080671\n",
      "Epoch: 66, Training RMSE: 1.063727, Test RMSE 1.077614\n",
      "Epoch: 67, Training RMSE: 1.059699, Test RMSE 1.074474\n",
      "Epoch: 68, Training RMSE: 1.055573, Test RMSE 1.071272\n",
      "Epoch: 69, Training RMSE: 1.051371, Test RMSE 1.068030\n",
      "Epoch: 70, Training RMSE: 1.047115, Test RMSE 1.064770\n",
      "Epoch: 71, Training RMSE: 1.042825, Test RMSE 1.061508\n",
      "Epoch: 72, Training RMSE: 1.038514, Test RMSE 1.058258\n",
      "Epoch: 73, Training RMSE: 1.034206, Test RMSE 1.055042\n",
      "Epoch: 74, Training RMSE: 1.029906, Test RMSE 1.051865\n",
      "Epoch: 75, Training RMSE: 1.025618, Test RMSE 1.048730\n",
      "Epoch: 76, Training RMSE: 1.021353, Test RMSE 1.045649\n",
      "Epoch: 77, Training RMSE: 1.017122, Test RMSE 1.042628\n",
      "Epoch: 78, Training RMSE: 1.012917, Test RMSE 1.039666\n",
      "Epoch: 79, Training RMSE: 1.008742, Test RMSE 1.036761\n",
      "Epoch: 80, Training RMSE: 1.004593, Test RMSE 1.033906\n",
      "Epoch: 81, Training RMSE: 1.000473, Test RMSE 1.031103\n",
      "Epoch: 82, Training RMSE: 0.996383, Test RMSE 1.028351\n",
      "Epoch: 83, Training RMSE: 0.992309, Test RMSE 1.025641\n",
      "Epoch: 84, Training RMSE: 0.988264, Test RMSE 1.022980\n",
      "Epoch: 85, Training RMSE: 0.984252, Test RMSE 1.020360\n",
      "Epoch: 86, Training RMSE: 0.980262, Test RMSE 1.017778\n",
      "Epoch: 87, Training RMSE: 0.976301, Test RMSE 1.015238\n",
      "Epoch: 88, Training RMSE: 0.972385, Test RMSE 1.012746\n",
      "Epoch: 89, Training RMSE: 0.968518, Test RMSE 1.010303\n",
      "Epoch: 90, Training RMSE: 0.964699, Test RMSE 1.007918\n",
      "Epoch: 91, Training RMSE: 0.960943, Test RMSE 1.005589\n",
      "Epoch: 92, Training RMSE: 0.957238, Test RMSE 1.003312\n",
      "Epoch: 93, Training RMSE: 0.953598, Test RMSE 1.001098\n",
      "Epoch: 94, Training RMSE: 0.950018, Test RMSE 0.998945\n",
      "Epoch: 95, Training RMSE: 0.946509, Test RMSE 0.996851\n",
      "Epoch: 96, Training RMSE: 0.943076, Test RMSE 0.994822\n",
      "Epoch: 97, Training RMSE: 0.939722, Test RMSE 0.992853\n",
      "Epoch: 98, Training RMSE: 0.936447, Test RMSE 0.990946\n",
      "Epoch: 99, Training RMSE: 0.933255, Test RMSE 0.989091\n",
      "Epoch: 100, Training RMSE: 0.930147, Test RMSE 0.987290\n",
      "Epoch: 101, Training RMSE: 0.927123, Test RMSE 0.985547\n",
      "Epoch: 102, Training RMSE: 0.924175, Test RMSE 0.983859\n",
      "Epoch: 103, Training RMSE: 0.921304, Test RMSE 0.982224\n",
      "Epoch: 104, Training RMSE: 0.918517, Test RMSE 0.980646\n",
      "Epoch: 105, Training RMSE: 0.915806, Test RMSE 0.979122\n",
      "Epoch: 106, Training RMSE: 0.913173, Test RMSE 0.977660\n",
      "Epoch: 107, Training RMSE: 0.910611, Test RMSE 0.976240\n",
      "Epoch: 108, Training RMSE: 0.908125, Test RMSE 0.974866\n",
      "Epoch: 109, Training RMSE: 0.905713, Test RMSE 0.973536\n",
      "Epoch: 110, Training RMSE: 0.903372, Test RMSE 0.972252\n",
      "Epoch: 111, Training RMSE: 0.901099, Test RMSE 0.971013\n",
      "Epoch: 112, Training RMSE: 0.898888, Test RMSE 0.969819\n",
      "Epoch: 113, Training RMSE: 0.896739, Test RMSE 0.968669\n",
      "Epoch: 114, Training RMSE: 0.894645, Test RMSE 0.967560\n",
      "Epoch: 115, Training RMSE: 0.892598, Test RMSE 0.966481\n",
      "Epoch: 116, Training RMSE: 0.890606, Test RMSE 0.965440\n",
      "Epoch: 117, Training RMSE: 0.888671, Test RMSE 0.964436\n",
      "Epoch: 118, Training RMSE: 0.886782, Test RMSE 0.963465\n",
      "Epoch: 119, Training RMSE: 0.884944, Test RMSE 0.962530\n",
      "Epoch: 120, Training RMSE: 0.883145, Test RMSE 0.961617\n",
      "Epoch: 121, Training RMSE: 0.881380, Test RMSE 0.960723\n",
      "Epoch: 122, Training RMSE: 0.879653, Test RMSE 0.959855\n",
      "Epoch: 123, Training RMSE: 0.877960, Test RMSE 0.959015\n",
      "Epoch: 124, Training RMSE: 0.876302, Test RMSE 0.958200\n",
      "Epoch: 125, Training RMSE: 0.874682, Test RMSE 0.957414\n",
      "Epoch: 126, Training RMSE: 0.873089, Test RMSE 0.956643\n",
      "Epoch: 127, Training RMSE: 0.871520, Test RMSE 0.955895\n",
      "Epoch: 128, Training RMSE: 0.869978, Test RMSE 0.955169\n",
      "Epoch: 129, Training RMSE: 0.868464, Test RMSE 0.954467\n",
      "Epoch: 130, Training RMSE: 0.866970, Test RMSE 0.953784\n",
      "Epoch: 131, Training RMSE: 0.865496, Test RMSE 0.953112\n",
      "Epoch: 132, Training RMSE: 0.864042, Test RMSE 0.952462\n",
      "Epoch: 133, Training RMSE: 0.862611, Test RMSE 0.951839\n",
      "Epoch: 134, Training RMSE: 0.861201, Test RMSE 0.951228\n",
      "Epoch: 135, Training RMSE: 0.859809, Test RMSE 0.950630\n",
      "Epoch: 136, Training RMSE: 0.858436, Test RMSE 0.950046\n",
      "Epoch: 137, Training RMSE: 0.857086, Test RMSE 0.949481\n",
      "Epoch: 138, Training RMSE: 0.855756, Test RMSE 0.948938\n",
      "Epoch: 139, Training RMSE: 0.854443, Test RMSE 0.948412\n",
      "Epoch: 140, Training RMSE: 0.853146, Test RMSE 0.947900\n",
      "Epoch: 141, Training RMSE: 0.851865, Test RMSE 0.947398\n",
      "Epoch: 142, Training RMSE: 0.850602, Test RMSE 0.946915\n",
      "Epoch: 143, Training RMSE: 0.849359, Test RMSE 0.946452\n",
      "Epoch: 144, Training RMSE: 0.848132, Test RMSE 0.945995\n",
      "Epoch: 145, Training RMSE: 0.846925, Test RMSE 0.945554\n",
      "Epoch: 146, Training RMSE: 0.845733, Test RMSE 0.945139\n",
      "Epoch: 147, Training RMSE: 0.844555, Test RMSE 0.944736\n",
      "Epoch: 148, Training RMSE: 0.843388, Test RMSE 0.944346\n",
      "Epoch: 149, Training RMSE: 0.842237, Test RMSE 0.943958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, Training RMSE: 0.841108, Test RMSE 0.943589\n",
      "Epoch: 151, Training RMSE: 0.839989, Test RMSE 0.943221\n",
      "Epoch: 152, Training RMSE: 0.838887, Test RMSE 0.942873\n",
      "Epoch: 153, Training RMSE: 0.837791, Test RMSE 0.942534\n",
      "Epoch: 154, Training RMSE: 0.836708, Test RMSE 0.942207\n",
      "Epoch: 155, Training RMSE: 0.835629, Test RMSE 0.941891\n",
      "Epoch: 156, Training RMSE: 0.834564, Test RMSE 0.941590\n",
      "Epoch: 157, Training RMSE: 0.833510, Test RMSE 0.941300\n",
      "Epoch: 158, Training RMSE: 0.832469, Test RMSE 0.941019\n",
      "Epoch: 159, Training RMSE: 0.831443, Test RMSE 0.940734\n",
      "Epoch: 160, Training RMSE: 0.830428, Test RMSE 0.940462\n",
      "Epoch: 161, Training RMSE: 0.829427, Test RMSE 0.940201\n",
      "Epoch: 162, Training RMSE: 0.828431, Test RMSE 0.939937\n",
      "Epoch: 163, Training RMSE: 0.827449, Test RMSE 0.939685\n",
      "Epoch: 164, Training RMSE: 0.826482, Test RMSE 0.939447\n",
      "Epoch: 165, Training RMSE: 0.825523, Test RMSE 0.939219\n",
      "Epoch: 166, Training RMSE: 0.824578, Test RMSE 0.939006\n",
      "Epoch: 167, Training RMSE: 0.823640, Test RMSE 0.938789\n",
      "Epoch: 168, Training RMSE: 0.822715, Test RMSE 0.938586\n",
      "Epoch: 169, Training RMSE: 0.821800, Test RMSE 0.938389\n",
      "Epoch: 170, Training RMSE: 0.820895, Test RMSE 0.938207\n",
      "Epoch: 171, Training RMSE: 0.820002, Test RMSE 0.938039\n",
      "Epoch: 172, Training RMSE: 0.819114, Test RMSE 0.937884\n",
      "Epoch: 173, Training RMSE: 0.818239, Test RMSE 0.937738\n",
      "Epoch: 174, Training RMSE: 0.817374, Test RMSE 0.937595\n",
      "Epoch: 175, Training RMSE: 0.816520, Test RMSE 0.937456\n",
      "Epoch: 176, Training RMSE: 0.815672, Test RMSE 0.937320\n",
      "Epoch: 177, Training RMSE: 0.814828, Test RMSE 0.937182\n",
      "Epoch: 178, Training RMSE: 0.813991, Test RMSE 0.937042\n",
      "Epoch: 179, Training RMSE: 0.813166, Test RMSE 0.936903\n",
      "Epoch: 180, Training RMSE: 0.812350, Test RMSE 0.936771\n",
      "Epoch: 181, Training RMSE: 0.811542, Test RMSE 0.936640\n",
      "Epoch: 182, Training RMSE: 0.810741, Test RMSE 0.936508\n",
      "Epoch: 183, Training RMSE: 0.809949, Test RMSE 0.936384\n",
      "Epoch: 184, Training RMSE: 0.809165, Test RMSE 0.936262\n",
      "Epoch: 185, Training RMSE: 0.808377, Test RMSE 0.936135\n",
      "Epoch: 186, Training RMSE: 0.807599, Test RMSE 0.936021\n",
      "Epoch: 187, Training RMSE: 0.806829, Test RMSE 0.935915\n",
      "Epoch: 188, Training RMSE: 0.806069, Test RMSE 0.935818\n",
      "Epoch: 189, Training RMSE: 0.805313, Test RMSE 0.935725\n",
      "Epoch: 190, Training RMSE: 0.804562, Test RMSE 0.935628\n",
      "Epoch: 191, Training RMSE: 0.803822, Test RMSE 0.935545\n",
      "Epoch: 192, Training RMSE: 0.803096, Test RMSE 0.935471\n",
      "Epoch: 193, Training RMSE: 0.802378, Test RMSE 0.935405\n",
      "Epoch: 194, Training RMSE: 0.801668, Test RMSE 0.935352\n",
      "Epoch: 195, Training RMSE: 0.800974, Test RMSE 0.935305\n",
      "Epoch: 196, Training RMSE: 0.800283, Test RMSE 0.935253\n",
      "Epoch: 197, Training RMSE: 0.799599, Test RMSE 0.935200\n",
      "Epoch: 198, Training RMSE: 0.798923, Test RMSE 0.935152\n",
      "Epoch: 199, Training RMSE: 0.798255, Test RMSE 0.935105\n",
      "Epoch: 200, Training RMSE: 0.797593, Test RMSE 0.935060\n",
      "Epoch: 201, Training RMSE: 0.796934, Test RMSE 0.935014\n",
      "Epoch: 202, Training RMSE: 0.796283, Test RMSE 0.934973\n",
      "Epoch: 203, Training RMSE: 0.795640, Test RMSE 0.934934\n",
      "Epoch: 204, Training RMSE: 0.795002, Test RMSE 0.934895\n",
      "Epoch: 205, Training RMSE: 0.794370, Test RMSE 0.934857\n",
      "Epoch: 206, Training RMSE: 0.793742, Test RMSE 0.934829\n",
      "Epoch: 207, Training RMSE: 0.793118, Test RMSE 0.934798\n",
      "Epoch: 208, Training RMSE: 0.792498, Test RMSE 0.934762\n",
      "Epoch: 209, Training RMSE: 0.791885, Test RMSE 0.934728\n",
      "Epoch: 210, Training RMSE: 0.791280, Test RMSE 0.934685\n",
      "Epoch: 211, Training RMSE: 0.790679, Test RMSE 0.934657\n",
      "Epoch: 212, Training RMSE: 0.790086, Test RMSE 0.934630\n",
      "Epoch: 213, Training RMSE: 0.789496, Test RMSE 0.934620\n",
      "Epoch: 214, Training RMSE: 0.788912, Test RMSE 0.934617\n",
      "Epoch: 215, Training RMSE: 0.788335, Test RMSE 0.934610\n",
      "Epoch: 216, Training RMSE: 0.787764, Test RMSE 0.934608\n",
      "Epoch: 217, Training RMSE: 0.787197, Test RMSE 0.934612\n",
      "Epoch: 218, Training RMSE: 0.786634, Test RMSE 0.934621\n",
      "Epoch: 219, Training RMSE: 0.786073, Test RMSE 0.934631\n",
      "Epoch: 220, Training RMSE: 0.785522, Test RMSE 0.934643\n",
      "Epoch: 221, Training RMSE: 0.784970, Test RMSE 0.934659\n",
      "Epoch: 222, Training RMSE: 0.784426, Test RMSE 0.934680\n",
      "Epoch: 223, Training RMSE: 0.783887, Test RMSE 0.934704\n",
      "Epoch: 224, Training RMSE: 0.783353, Test RMSE 0.934718\n",
      "Epoch: 225, Training RMSE: 0.782821, Test RMSE 0.934724\n",
      "Epoch: 226, Training RMSE: 0.782297, Test RMSE 0.934730\n",
      "Epoch: 227, Training RMSE: 0.781778, Test RMSE 0.934749\n",
      "Epoch: 228, Training RMSE: 0.781263, Test RMSE 0.934767\n",
      "Epoch: 229, Training RMSE: 0.780754, Test RMSE 0.934775\n",
      "Epoch: 230, Training RMSE: 0.780252, Test RMSE 0.934781\n",
      "Epoch: 231, Training RMSE: 0.779762, Test RMSE 0.934796\n",
      "Epoch: 232, Training RMSE: 0.779275, Test RMSE 0.934821\n",
      "Epoch: 233, Training RMSE: 0.778792, Test RMSE 0.934843\n",
      "Epoch: 234, Training RMSE: 0.778312, Test RMSE 0.934862\n",
      "Epoch: 235, Training RMSE: 0.777835, Test RMSE 0.934878\n",
      "Epoch: 236, Training RMSE: 0.777360, Test RMSE 0.934892\n",
      "Epoch: 237, Training RMSE: 0.776894, Test RMSE 0.934915\n",
      "Epoch: 238, Training RMSE: 0.776430, Test RMSE 0.934940\n",
      "Epoch: 239, Training RMSE: 0.775968, Test RMSE 0.934949\n",
      "Epoch: 240, Training RMSE: 0.775513, Test RMSE 0.934958\n",
      "Epoch: 241, Training RMSE: 0.775059, Test RMSE 0.934969\n",
      "Epoch: 242, Training RMSE: 0.774610, Test RMSE 0.934989\n",
      "Epoch: 243, Training RMSE: 0.774165, Test RMSE 0.935018\n",
      "Epoch: 244, Training RMSE: 0.773728, Test RMSE 0.935054\n",
      "Epoch: 245, Training RMSE: 0.773296, Test RMSE 0.935087\n",
      "Epoch: 246, Training RMSE: 0.772862, Test RMSE 0.935121\n",
      "Epoch: 247, Training RMSE: 0.772428, Test RMSE 0.935155\n",
      "Epoch: 248, Training RMSE: 0.772003, Test RMSE 0.935196\n",
      "Epoch: 249, Training RMSE: 0.771582, Test RMSE 0.935229\n",
      "Epoch: 250, Training RMSE: 0.771162, Test RMSE 0.935263\n",
      "Epoch: 251, Training RMSE: 0.770749, Test RMSE 0.935303\n",
      "Epoch: 252, Training RMSE: 0.770344, Test RMSE 0.935340\n",
      "Epoch: 253, Training RMSE: 0.769941, Test RMSE 0.935373\n",
      "Epoch: 254, Training RMSE: 0.769544, Test RMSE 0.935407\n",
      "Epoch: 255, Training RMSE: 0.769150, Test RMSE 0.935440\n",
      "Epoch: 256, Training RMSE: 0.768756, Test RMSE 0.935477\n",
      "Epoch: 257, Training RMSE: 0.768364, Test RMSE 0.935512\n",
      "Epoch: 258, Training RMSE: 0.767969, Test RMSE 0.935540\n",
      "Epoch: 259, Training RMSE: 0.767578, Test RMSE 0.935567\n",
      "Epoch: 260, Training RMSE: 0.767190, Test RMSE 0.935599\n",
      "Epoch: 261, Training RMSE: 0.766804, Test RMSE 0.935636\n",
      "Epoch: 262, Training RMSE: 0.766420, Test RMSE 0.935678\n",
      "Epoch: 263, Training RMSE: 0.766042, Test RMSE 0.935726\n",
      "Epoch: 264, Training RMSE: 0.765671, Test RMSE 0.935773\n",
      "Epoch: 265, Training RMSE: 0.765303, Test RMSE 0.935818\n",
      "Epoch: 266, Training RMSE: 0.764939, Test RMSE 0.935865\n",
      "Epoch: 267, Training RMSE: 0.764578, Test RMSE 0.935900\n",
      "Epoch: 268, Training RMSE: 0.764222, Test RMSE 0.935935\n",
      "Epoch: 269, Training RMSE: 0.763869, Test RMSE 0.935970\n",
      "Epoch: 270, Training RMSE: 0.763521, Test RMSE 0.936004\n",
      "Epoch: 271, Training RMSE: 0.763172, Test RMSE 0.936041\n",
      "Epoch: 272, Training RMSE: 0.762823, Test RMSE 0.936067\n",
      "Epoch: 273, Training RMSE: 0.762478, Test RMSE 0.936103\n",
      "Epoch: 274, Training RMSE: 0.762135, Test RMSE 0.936137\n",
      "Epoch: 275, Training RMSE: 0.761799, Test RMSE 0.936173\n",
      "Epoch: 276, Training RMSE: 0.761461, Test RMSE 0.936210\n",
      "Epoch: 277, Training RMSE: 0.761125, Test RMSE 0.936252\n",
      "Epoch: 278, Training RMSE: 0.760797, Test RMSE 0.936300\n",
      "Epoch: 279, Training RMSE: 0.760474, Test RMSE 0.936351\n",
      "Epoch: 280, Training RMSE: 0.760158, Test RMSE 0.936408\n",
      "Epoch: 281, Training RMSE: 0.759842, Test RMSE 0.936464\n",
      "Epoch: 282, Training RMSE: 0.759527, Test RMSE 0.936502\n",
      "Epoch: 283, Training RMSE: 0.759212, Test RMSE 0.936542\n",
      "Epoch: 284, Training RMSE: 0.758899, Test RMSE 0.936582\n",
      "Epoch: 285, Training RMSE: 0.758585, Test RMSE 0.936633\n",
      "Epoch: 286, Training RMSE: 0.758276, Test RMSE 0.936673\n",
      "Epoch: 287, Training RMSE: 0.757973, Test RMSE 0.936711\n",
      "Epoch: 288, Training RMSE: 0.757672, Test RMSE 0.936743\n",
      "Epoch: 289, Training RMSE: 0.757368, Test RMSE 0.936773\n",
      "Epoch: 290, Training RMSE: 0.757067, Test RMSE 0.936810\n",
      "Epoch: 291, Training RMSE: 0.756765, Test RMSE 0.936855\n",
      "Epoch: 292, Training RMSE: 0.756469, Test RMSE 0.936886\n",
      "Epoch: 293, Training RMSE: 0.756177, Test RMSE 0.936924\n",
      "Epoch: 294, Training RMSE: 0.755887, Test RMSE 0.936960\n",
      "Epoch: 295, Training RMSE: 0.755598, Test RMSE 0.936982\n",
      "Epoch: 296, Training RMSE: 0.755316, Test RMSE 0.937009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 297, Training RMSE: 0.755027, Test RMSE 0.937041\n",
      "Epoch: 298, Training RMSE: 0.754747, Test RMSE 0.937084\n",
      "Epoch: 299, Training RMSE: 0.754472, Test RMSE 0.937128\n",
      "Epoch: 300, Training RMSE: 0.754198, Test RMSE 0.937162\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "pmf.fit(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (300,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a3a7c6a0d0f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmse_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmse_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The MovieLens Dataset Learning Curve'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RMSE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2838\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2839\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2840\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2841\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \"\"\"\n\u001b[1;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (300,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(pmf.maxepoch), pmf.rmse_train, marker='o', label='Training Data')\n",
    "plt.plot(range(pmf.maxepoch), pmf.rmse_test, marker='v', label='Test Data')\n",
    "plt.title('The MovieLens Dataset Learning Curve')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(\"precision_acc,recall_acc:\" + str(pmf.topK(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMF_torch(object):\n",
    "    def __init__(self, num_feat=10, \n",
    "                 epsilon=1, \n",
    "                 _lambda=0.1, \n",
    "                 momentum=0.8, \n",
    "                 maxepoch=20, \n",
    "                 num_batches=10, \n",
    "                 batch_size=1000):\n",
    "        self.num_feat = num_feat  # Number of latent features,\n",
    "        self.epsilon = epsilon  # learning rate,\n",
    "        self._lambda = _lambda  # L2 regularization,\n",
    "        self.momentum = momentum  # momentum of the gradient,\n",
    "        self.maxepoch = maxepoch  # Number of epoch before stop,\n",
    "        self.num_batches = num_batches  # Number of batches in each epoch (for SGD optimization),\n",
    "        self.batch_size = batch_size  # Number of training samples used in each batches (for SGD optimization)\n",
    "\n",
    "        self.w_Item = None  # Item feature vectors\n",
    "        self.w_User = None  # User feature vectors\n",
    "\n",
    "        self.rmse_train = []\n",
    "        self.rmse_test = []\n",
    "\n",
    "    def loss(self, params, ratings, partition, use_regularize = True, log_prob = False):\n",
    "        pred_out = torch.sum(torch.multiply(params[:partition],\n",
    "                                            params[partition:]),\n",
    "                                     axis=1)  # mean_inv subtracted # np.multiply对应位置元素相乘\n",
    "\n",
    "        rawErr = pred_out - ratings + self.mean_inv\n",
    "        \n",
    "        loss = torch.norm(rawErr) ** 2 \n",
    "        if use_regularize:\n",
    "            loss += 0.5 * self._lambda * (torch.norm(params) ** 2)\n",
    "        if log_prob:\n",
    "            loss = -loss\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    # ***Fit the model with train_tuple and evaluate RMSE on both train and test data.  ***********#\n",
    "    # ***************** train_vec=TrainData, test_vec=TestData*************#\n",
    "    def fit(self, train_vec, test_vec):\n",
    "        # mean subtraction\n",
    "        self.mean_inv = torch.mean(train_vec[:, 2])  # 评分平均值\n",
    "\n",
    "        pairs_train = train_vec.shape[0]  # traindata 中条目数\n",
    "        pairs_test = test_vec.shape[0]  # testdata中条目数\n",
    "\n",
    "        # 1-p-i, 2-m-c\n",
    "        num_user = int(max(torch.amax(train_vec[:, 0]), torch.amax(test_vec[:, 0]))) + 1  # 第0列，user总数\n",
    "        num_item = int(max(torch.amax(train_vec[:, 1]), torch.amax(test_vec[:, 1]))) + 1  # 第1列，movie总数\n",
    "\n",
    "        incremental = False  # 增量\n",
    "        if ((not incremental) or (self.w_Item is None)):\n",
    "            # initialize\n",
    "            self.epoch = 0\n",
    "            self.w_Item = 0.1 * torch.randn(num_item, self.num_feat)  # numpy.random.randn 电影 M x D 正态分布矩阵\n",
    "            self.w_User = 0.1 * torch.randn(num_user, self.num_feat)  # numpy.random.randn 用户 N x D 正态分布矩阵\n",
    "\n",
    "            self.w_Item_inc = torch.zeros((num_item, self.num_feat))  # 创建电影 M x D 0矩阵\n",
    "            self.w_User_inc = torch.zeros((num_user, self.num_feat))  # 创建用户 N x D 0矩阵\n",
    "\n",
    "        while self.epoch < self.maxepoch:  # 检查迭代次数\n",
    "            self.epoch += 1\n",
    "\n",
    "            # Shuffle training truples\n",
    "            shuffled_order = np.arange(train_vec.shape[0])  # 根据记录数创建等差array\n",
    "            np.random.shuffle(shuffled_order)  # 用于将一个列表中的元素打乱\n",
    "\n",
    "            # Batch update\n",
    "            for batch in range(self.num_batches):  # 每次迭代要使用的数据量\n",
    "                # print \"epoch %d batch %d\" % (self.epoch, batch+1)\n",
    "                \n",
    "                test = np.arange(self.batch_size * batch, self.batch_size * (batch + 1))\n",
    "                batch_idx = np.mod(test, shuffled_order.shape[0])  # 本次迭代要使用的索引下标\n",
    "\n",
    "                batch_UserID = np.array(train_vec[shuffled_order[batch_idx], 0], dtype='int32')\n",
    "                batch_ItemID = np.array(train_vec[shuffled_order[batch_idx], 1], dtype='int32')\n",
    "                \n",
    "                w_User_batch = self.w_User[batch_UserID, :]\n",
    "                w_Item_batch = self.w_Item[batch_ItemID, :]\n",
    "                concat_shape = w_User_batch.shape[0]\n",
    "                \n",
    "                w_big = torch.cat((w_User_batch.detach(), w_Item_batch.detach())).requires_grad_()\n",
    "\n",
    "                # Compute gradients\n",
    "                ratings = train_vec[shuffled_order[batch_idx], 2]\n",
    "                loss = self.loss(w_big, ratings, concat_shape)\n",
    "                \n",
    "                Ix_w_big = torch.autograd.grad(loss, w_big)[0]\n",
    "                Ix_User = Ix_w_big[:concat_shape]\n",
    "                Ix_Item = Ix_w_big[concat_shape:]\n",
    "                \n",
    "                #Ix_User = 2 * torch.multiply(rawErr[:, np.newaxis], self.w_Item[batch_ItemID, :]) \\\n",
    "                #       + self._lambda * self.w_User[batch_UserID, :]\n",
    "                #Ix_Item = 2 * torch.multiply(rawErr[:, np.newaxis], self.w_User[batch_UserID, :]) \\\n",
    "                #       + self._lambda * (self.w_Item[batch_ItemID, :])  # np.newaxis :increase the dimension\n",
    "\n",
    "                dw_Item = torch.zeros((num_item, self.num_feat))\n",
    "                dw_User = torch.zeros((num_user, self.num_feat))\n",
    "\n",
    "                # loop to aggreate the gradients of the same element\n",
    "                for i in range(self.batch_size):\n",
    "                    dw_Item[batch_ItemID[i], :] += Ix_Item[i, :]\n",
    "                    dw_User[batch_UserID[i], :] += Ix_User[i, :]\n",
    "\n",
    "                # Update with momentum\n",
    "                self.w_Item_inc = self.momentum * self.w_Item_inc + self.epsilon * dw_Item / self.batch_size\n",
    "                self.w_User_inc = self.momentum * self.w_User_inc + self.epsilon * dw_User / self.batch_size\n",
    "\n",
    "                self.w_Item = self.w_Item - self.w_Item_inc\n",
    "                self.w_User = self.w_User - self.w_User_inc\n",
    "\n",
    "                # Compute Objective Function after\n",
    "                if batch == self.num_batches - 1:\n",
    "                    ratings = train_vec[:, 2]\n",
    "                    concat_shape = self.w_User[np.array(train_vec[:, 0], dtype='int32'), :].shape[0]\n",
    "                    w_big = torch.cat((self.w_User[np.array(train_vec[:, 0], dtype='int32'), :], \n",
    "                                       self.w_Item[np.array(train_vec[:, 1], dtype='int32'), :]))\n",
    "                    obj = self.loss(w_big, ratings, concat_shape, False).item()\n",
    "                    obj += 0.5 * self._lambda * (torch.norm(self.w_User) ** 2 + torch.norm(self.w_Item) ** 2)\n",
    "\n",
    "                    self.rmse_train.append((obj / pairs_train) ** 0.5)\n",
    "\n",
    "                # Compute validation error\n",
    "                if batch == self.num_batches - 1:\n",
    "                    ratings = test_vec[:, 2]\n",
    "                    concat_shape = self.w_User[np.array(test_vec[:, 0], dtype='int32'), :].shape[0]\n",
    "                    w_big = torch.cat((self.w_User[np.array(test_vec[:, 0], dtype='int32'), :], \n",
    "                                       self.w_Item[np.array(test_vec[:, 1], dtype='int32'), :]))\n",
    "                  \n",
    "                    loss = self.loss(w_big, ratings, concat_shape, use_regularize = False).item()\n",
    "                    self.rmse_test.append((loss / pairs_test)** 0.5)\n",
    "\n",
    "                    # Print info\n",
    "                    if batch == self.num_batches - 1:\n",
    "                        print('Epoch: %d, Training RMSE: %f, Test RMSE %f' % (self.epoch,\n",
    "                                                                              self.rmse_train[-1], \n",
    "                                                                              self.rmse_test[-1]))\n",
    "\n",
    "    def predict(self, invID):\n",
    "        return (self.w_Item @ self.w_User[int(invID), :]) + self.mean_inv  # numpy.dot 点乘\n",
    "\n",
    "    # ****************Set parameters by providing a parameter dictionary.  ***********#\n",
    "    def set_params(self, parameters):\n",
    "        if isinstance(parameters, dict):\n",
    "            self.num_feat = parameters.get(\"num_feat\", 10)\n",
    "            self.epsilon = parameters.get(\"epsilon\", 1)\n",
    "            self._lambda = parameters.get(\"_lambda\", 0.1)\n",
    "            self.momentum = parameters.get(\"momentum\", 0.8)\n",
    "            self.maxepoch = parameters.get(\"maxepoch\", 20)\n",
    "            self.num_batches = parameters.get(\"num_batches\", 10)\n",
    "            self.batch_size = parameters.get(\"batch_size\", 1000)\n",
    "\n",
    "    def topK(self, test_vec, k=10):\n",
    "        inv_lst = np.unique(test_vec[:, 0])\n",
    "        pred = {}\n",
    "        for inv in inv_lst:\n",
    "            if pred.get(inv, None) is None:\n",
    "                pred[inv] = np.argsort(self.predict(inv).numpy())[-k:]  # numpy.argsort索引排序\n",
    "\n",
    "        intersection_cnt = {}\n",
    "        for i in range(test_vec.shape[0]):\n",
    "            if test_vec[i, 1] in pred[test_vec[i, 0]]:\n",
    "                intersection_cnt[test_vec[i, 0]] = intersection_cnt.get(test_vec[i, 0], 0) + 1\n",
    "        invPairs_cnt = np.bincount(np.array(test_vec[:, 0], dtype='int32'))\n",
    "\n",
    "        precision_acc = 0.0\n",
    "        recall_acc = 0.0\n",
    "        for inv in inv_lst:\n",
    "            precision_acc += intersection_cnt.get(inv, 0) / float(k)\n",
    "            recall_acc += intersection_cnt.get(inv, 0) / float(invPairs_cnt[int(inv)])\n",
    "\n",
    "        return precision_acc / len(inv_lst), recall_acc / len(inv_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_pmf = PMF_torch()\n",
    "torch_pmf.set_params({\"num_feat\": 10, \n",
    "                      \"epsilon\": 10, \n",
    "                      \"_lambda\": 0.1, \n",
    "                      \"momentum\": 0.8, \n",
    "                      \"maxepoch\": 300, \n",
    "                      \"num_batches\": 1,\n",
    "                      \"batch_size\": 100000})\n",
    "torch_train = torch.FloatTensor(train)\n",
    "torch_test = torch.FloatTensor(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training RMSE: 1.126533, Test RMSE 1.124365\n",
      "Epoch: 2, Training RMSE: 1.126426, Test RMSE 1.124359\n",
      "Epoch: 3, Training RMSE: 1.126284, Test RMSE 1.124353\n",
      "Epoch: 4, Training RMSE: 1.126112, Test RMSE 1.124341\n",
      "Epoch: 5, Training RMSE: 1.125922, Test RMSE 1.124330\n",
      "Epoch: 6, Training RMSE: 1.125716, Test RMSE 1.124317\n",
      "Epoch: 7, Training RMSE: 1.125502, Test RMSE 1.124304\n",
      "Epoch: 8, Training RMSE: 1.125280, Test RMSE 1.124286\n",
      "Epoch: 9, Training RMSE: 1.125055, Test RMSE 1.124270\n",
      "Epoch: 10, Training RMSE: 1.124827, Test RMSE 1.124253\n",
      "Epoch: 11, Training RMSE: 1.124598, Test RMSE 1.124234\n",
      "Epoch: 12, Training RMSE: 1.124369, Test RMSE 1.124210\n",
      "Epoch: 13, Training RMSE: 1.124139, Test RMSE 1.124189\n",
      "Epoch: 14, Training RMSE: 1.123906, Test RMSE 1.124157\n",
      "Epoch: 15, Training RMSE: 1.123674, Test RMSE 1.124126\n",
      "Epoch: 16, Training RMSE: 1.123438, Test RMSE 1.124090\n",
      "Epoch: 17, Training RMSE: 1.123202, Test RMSE 1.124050\n",
      "Epoch: 18, Training RMSE: 1.122961, Test RMSE 1.124004\n",
      "Epoch: 19, Training RMSE: 1.122714, Test RMSE 1.123955\n",
      "Epoch: 20, Training RMSE: 1.122462, Test RMSE 1.123896\n",
      "Epoch: 21, Training RMSE: 1.122204, Test RMSE 1.123831\n",
      "Epoch: 22, Training RMSE: 1.121937, Test RMSE 1.123756\n",
      "Epoch: 23, Training RMSE: 1.121660, Test RMSE 1.123672\n",
      "Epoch: 24, Training RMSE: 1.121371, Test RMSE 1.123578\n",
      "Epoch: 25, Training RMSE: 1.121070, Test RMSE 1.123472\n",
      "Epoch: 26, Training RMSE: 1.120753, Test RMSE 1.123350\n",
      "Epoch: 27, Training RMSE: 1.120419, Test RMSE 1.123214\n",
      "Epoch: 28, Training RMSE: 1.120064, Test RMSE 1.123061\n",
      "Epoch: 29, Training RMSE: 1.119688, Test RMSE 1.122888\n",
      "Epoch: 30, Training RMSE: 1.119285, Test RMSE 1.122698\n",
      "Epoch: 31, Training RMSE: 1.118854, Test RMSE 1.122481\n",
      "Epoch: 32, Training RMSE: 1.118394, Test RMSE 1.122238\n",
      "Epoch: 33, Training RMSE: 1.117897, Test RMSE 1.121964\n",
      "Epoch: 34, Training RMSE: 1.117360, Test RMSE 1.121657\n",
      "Epoch: 35, Training RMSE: 1.116782, Test RMSE 1.121317\n",
      "Epoch: 36, Training RMSE: 1.116155, Test RMSE 1.120936\n",
      "Epoch: 37, Training RMSE: 1.115475, Test RMSE 1.120508\n",
      "Epoch: 38, Training RMSE: 1.114736, Test RMSE 1.120033\n",
      "Epoch: 39, Training RMSE: 1.113931, Test RMSE 1.119506\n",
      "Epoch: 40, Training RMSE: 1.113055, Test RMSE 1.118917\n",
      "Epoch: 41, Training RMSE: 1.112101, Test RMSE 1.118260\n",
      "Epoch: 42, Training RMSE: 1.111059, Test RMSE 1.117529\n",
      "Epoch: 43, Training RMSE: 1.109927, Test RMSE 1.116721\n",
      "Epoch: 44, Training RMSE: 1.108694, Test RMSE 1.115826\n",
      "Epoch: 45, Training RMSE: 1.107351, Test RMSE 1.114842\n",
      "Epoch: 46, Training RMSE: 1.105886, Test RMSE 1.113755\n",
      "Epoch: 47, Training RMSE: 1.104294, Test RMSE 1.112560\n",
      "Epoch: 48, Training RMSE: 1.102563, Test RMSE 1.111246\n",
      "Epoch: 49, Training RMSE: 1.100684, Test RMSE 1.109808\n",
      "Epoch: 50, Training RMSE: 1.098650, Test RMSE 1.108238\n",
      "Epoch: 51, Training RMSE: 1.096452, Test RMSE 1.106522\n",
      "Epoch: 52, Training RMSE: 1.094082, Test RMSE 1.104666\n",
      "Epoch: 53, Training RMSE: 1.091532, Test RMSE 1.102661\n",
      "Epoch: 54, Training RMSE: 1.088797, Test RMSE 1.100500\n",
      "Epoch: 55, Training RMSE: 1.085874, Test RMSE 1.098177\n",
      "Epoch: 56, Training RMSE: 1.082765, Test RMSE 1.095702\n",
      "Epoch: 57, Training RMSE: 1.079469, Test RMSE 1.093078\n",
      "Epoch: 58, Training RMSE: 1.075993, Test RMSE 1.090309\n",
      "Epoch: 59, Training RMSE: 1.072342, Test RMSE 1.087402\n",
      "Epoch: 60, Training RMSE: 1.068530, Test RMSE 1.084370\n",
      "Epoch: 61, Training RMSE: 1.064575, Test RMSE 1.081228\n",
      "Epoch: 62, Training RMSE: 1.060491, Test RMSE 1.078003\n",
      "Epoch: 63, Training RMSE: 1.056296, Test RMSE 1.074700\n",
      "Epoch: 64, Training RMSE: 1.052021, Test RMSE 1.071351\n",
      "Epoch: 65, Training RMSE: 1.047688, Test RMSE 1.067984\n",
      "Epoch: 66, Training RMSE: 1.043321, Test RMSE 1.064613\n",
      "Epoch: 67, Training RMSE: 1.038930, Test RMSE 1.061255\n",
      "Epoch: 68, Training RMSE: 1.034542, Test RMSE 1.057935\n",
      "Epoch: 69, Training RMSE: 1.030166, Test RMSE 1.054660\n",
      "Epoch: 70, Training RMSE: 1.025815, Test RMSE 1.051447\n",
      "Epoch: 71, Training RMSE: 1.021502, Test RMSE 1.048300\n",
      "Epoch: 72, Training RMSE: 1.017227, Test RMSE 1.045221\n",
      "Epoch: 73, Training RMSE: 1.012994, Test RMSE 1.042202\n",
      "Epoch: 74, Training RMSE: 1.008802, Test RMSE 1.039240\n",
      "Epoch: 75, Training RMSE: 1.004653, Test RMSE 1.036349\n",
      "Epoch: 76, Training RMSE: 1.000536, Test RMSE 1.033506\n",
      "Epoch: 77, Training RMSE: 0.996455, Test RMSE 1.030715\n",
      "Epoch: 78, Training RMSE: 0.992406, Test RMSE 1.027965\n",
      "Epoch: 79, Training RMSE: 0.988392, Test RMSE 1.025259\n",
      "Epoch: 80, Training RMSE: 0.984414, Test RMSE 1.022597\n",
      "Epoch: 81, Training RMSE: 0.980461, Test RMSE 1.019967\n",
      "Epoch: 82, Training RMSE: 0.976537, Test RMSE 1.017370\n",
      "Epoch: 83, Training RMSE: 0.972652, Test RMSE 1.014817\n",
      "Epoch: 84, Training RMSE: 0.968809, Test RMSE 1.012314\n",
      "Epoch: 85, Training RMSE: 0.965011, Test RMSE 1.009858\n",
      "Epoch: 86, Training RMSE: 0.961264, Test RMSE 1.007451\n",
      "Epoch: 87, Training RMSE: 0.957579, Test RMSE 1.005098\n",
      "Epoch: 88, Training RMSE: 0.953955, Test RMSE 1.002799\n",
      "Epoch: 89, Training RMSE: 0.950387, Test RMSE 1.000555\n",
      "Epoch: 90, Training RMSE: 0.946894, Test RMSE 0.998374\n",
      "Epoch: 91, Training RMSE: 0.943468, Test RMSE 0.996251\n",
      "Epoch: 92, Training RMSE: 0.940124, Test RMSE 0.994193\n",
      "Epoch: 93, Training RMSE: 0.936857, Test RMSE 0.992199\n",
      "Epoch: 94, Training RMSE: 0.933677, Test RMSE 0.990278\n",
      "Epoch: 95, Training RMSE: 0.930584, Test RMSE 0.988426\n",
      "Epoch: 96, Training RMSE: 0.927579, Test RMSE 0.986642\n",
      "Epoch: 97, Training RMSE: 0.924655, Test RMSE 0.984915\n",
      "Epoch: 98, Training RMSE: 0.921821, Test RMSE 0.983251\n",
      "Epoch: 99, Training RMSE: 0.919079, Test RMSE 0.981660\n",
      "Epoch: 100, Training RMSE: 0.916427, Test RMSE 0.980128\n",
      "Epoch: 101, Training RMSE: 0.913859, Test RMSE 0.978652\n",
      "Epoch: 102, Training RMSE: 0.911370, Test RMSE 0.977237\n",
      "Epoch: 103, Training RMSE: 0.908967, Test RMSE 0.975874\n",
      "Epoch: 104, Training RMSE: 0.906635, Test RMSE 0.974572\n",
      "Epoch: 105, Training RMSE: 0.904373, Test RMSE 0.973331\n",
      "Epoch: 106, Training RMSE: 0.902179, Test RMSE 0.972135\n",
      "Epoch: 107, Training RMSE: 0.900048, Test RMSE 0.970983\n",
      "Epoch: 108, Training RMSE: 0.897969, Test RMSE 0.969872\n",
      "Epoch: 109, Training RMSE: 0.895959, Test RMSE 0.968806\n",
      "Epoch: 110, Training RMSE: 0.894002, Test RMSE 0.967781\n",
      "Epoch: 111, Training RMSE: 0.892088, Test RMSE 0.966790\n",
      "Epoch: 112, Training RMSE: 0.890217, Test RMSE 0.965826\n",
      "Epoch: 113, Training RMSE: 0.888390, Test RMSE 0.964898\n",
      "Epoch: 114, Training RMSE: 0.886609, Test RMSE 0.963997\n",
      "Epoch: 115, Training RMSE: 0.884871, Test RMSE 0.963132\n",
      "Epoch: 116, Training RMSE: 0.883168, Test RMSE 0.962289\n",
      "Epoch: 117, Training RMSE: 0.881500, Test RMSE 0.961469\n",
      "Epoch: 118, Training RMSE: 0.879866, Test RMSE 0.960668\n",
      "Epoch: 119, Training RMSE: 0.878260, Test RMSE 0.959899\n",
      "Epoch: 120, Training RMSE: 0.876674, Test RMSE 0.959155\n",
      "Epoch: 121, Training RMSE: 0.875120, Test RMSE 0.958434\n",
      "Epoch: 122, Training RMSE: 0.873590, Test RMSE 0.957741\n",
      "Epoch: 123, Training RMSE: 0.872085, Test RMSE 0.957068\n",
      "Epoch: 124, Training RMSE: 0.870604, Test RMSE 0.956411\n",
      "Epoch: 125, Training RMSE: 0.869156, Test RMSE 0.955783\n",
      "Epoch: 126, Training RMSE: 0.867734, Test RMSE 0.955185\n",
      "Epoch: 127, Training RMSE: 0.866332, Test RMSE 0.954605\n",
      "Epoch: 128, Training RMSE: 0.864952, Test RMSE 0.954048\n",
      "Epoch: 129, Training RMSE: 0.863591, Test RMSE 0.953506\n",
      "Epoch: 130, Training RMSE: 0.862241, Test RMSE 0.952976\n",
      "Epoch: 131, Training RMSE: 0.860911, Test RMSE 0.952468\n",
      "Epoch: 132, Training RMSE: 0.859602, Test RMSE 0.951969\n",
      "Epoch: 133, Training RMSE: 0.858312, Test RMSE 0.951486\n",
      "Epoch: 134, Training RMSE: 0.857037, Test RMSE 0.951023\n",
      "Epoch: 135, Training RMSE: 0.855775, Test RMSE 0.950565\n",
      "Epoch: 136, Training RMSE: 0.854521, Test RMSE 0.950124\n",
      "Epoch: 137, Training RMSE: 0.853289, Test RMSE 0.949698\n",
      "Epoch: 138, Training RMSE: 0.852074, Test RMSE 0.949292\n",
      "Epoch: 139, Training RMSE: 0.850872, Test RMSE 0.948895\n",
      "Epoch: 140, Training RMSE: 0.849692, Test RMSE 0.948508\n",
      "Epoch: 141, Training RMSE: 0.848523, Test RMSE 0.948135\n",
      "Epoch: 142, Training RMSE: 0.847369, Test RMSE 0.947785\n",
      "Epoch: 143, Training RMSE: 0.846226, Test RMSE 0.947454\n",
      "Epoch: 144, Training RMSE: 0.845096, Test RMSE 0.947128\n",
      "Epoch: 145, Training RMSE: 0.843980, Test RMSE 0.946808\n",
      "Epoch: 146, Training RMSE: 0.842881, Test RMSE 0.946499\n",
      "Epoch: 147, Training RMSE: 0.841789, Test RMSE 0.946197\n",
      "Epoch: 148, Training RMSE: 0.840705, Test RMSE 0.945896\n",
      "Epoch: 149, Training RMSE: 0.839631, Test RMSE 0.945609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, Training RMSE: 0.838568, Test RMSE 0.945331\n",
      "Epoch: 151, Training RMSE: 0.837515, Test RMSE 0.945053\n",
      "Epoch: 152, Training RMSE: 0.836475, Test RMSE 0.944782\n",
      "Epoch: 153, Training RMSE: 0.835444, Test RMSE 0.944512\n",
      "Epoch: 154, Training RMSE: 0.834426, Test RMSE 0.944251\n",
      "Epoch: 155, Training RMSE: 0.833421, Test RMSE 0.944007\n",
      "Epoch: 156, Training RMSE: 0.832422, Test RMSE 0.943772\n",
      "Epoch: 157, Training RMSE: 0.831439, Test RMSE 0.943535\n",
      "Epoch: 158, Training RMSE: 0.830467, Test RMSE 0.943304\n",
      "Epoch: 159, Training RMSE: 0.829497, Test RMSE 0.943080\n",
      "Epoch: 160, Training RMSE: 0.828538, Test RMSE 0.942861\n",
      "Epoch: 161, Training RMSE: 0.827584, Test RMSE 0.942643\n",
      "Epoch: 162, Training RMSE: 0.826639, Test RMSE 0.942449\n",
      "Epoch: 163, Training RMSE: 0.825705, Test RMSE 0.942273\n",
      "Epoch: 164, Training RMSE: 0.824777, Test RMSE 0.942100\n",
      "Epoch: 165, Training RMSE: 0.823859, Test RMSE 0.941944\n",
      "Epoch: 166, Training RMSE: 0.822947, Test RMSE 0.941798\n",
      "Epoch: 167, Training RMSE: 0.822044, Test RMSE 0.941666\n",
      "Epoch: 168, Training RMSE: 0.821148, Test RMSE 0.941536\n",
      "Epoch: 169, Training RMSE: 0.820259, Test RMSE 0.941414\n",
      "Epoch: 170, Training RMSE: 0.819383, Test RMSE 0.941283\n",
      "Epoch: 171, Training RMSE: 0.818514, Test RMSE 0.941141\n",
      "Epoch: 172, Training RMSE: 0.817655, Test RMSE 0.941002\n",
      "Epoch: 173, Training RMSE: 0.816805, Test RMSE 0.940862\n",
      "Epoch: 174, Training RMSE: 0.815965, Test RMSE 0.940728\n",
      "Epoch: 175, Training RMSE: 0.815135, Test RMSE 0.940597\n",
      "Epoch: 176, Training RMSE: 0.814311, Test RMSE 0.940482\n",
      "Epoch: 177, Training RMSE: 0.813496, Test RMSE 0.940375\n",
      "Epoch: 178, Training RMSE: 0.812686, Test RMSE 0.940264\n",
      "Epoch: 179, Training RMSE: 0.811885, Test RMSE 0.940160\n",
      "Epoch: 180, Training RMSE: 0.811093, Test RMSE 0.940072\n",
      "Epoch: 181, Training RMSE: 0.810308, Test RMSE 0.939982\n",
      "Epoch: 182, Training RMSE: 0.809536, Test RMSE 0.939905\n",
      "Epoch: 183, Training RMSE: 0.808766, Test RMSE 0.939828\n",
      "Epoch: 184, Training RMSE: 0.808001, Test RMSE 0.939745\n",
      "Epoch: 185, Training RMSE: 0.807248, Test RMSE 0.939666\n",
      "Epoch: 186, Training RMSE: 0.806497, Test RMSE 0.939598\n",
      "Epoch: 187, Training RMSE: 0.805751, Test RMSE 0.939536\n",
      "Epoch: 188, Training RMSE: 0.805010, Test RMSE 0.939475\n",
      "Epoch: 189, Training RMSE: 0.804278, Test RMSE 0.939415\n",
      "Epoch: 190, Training RMSE: 0.803552, Test RMSE 0.939345\n",
      "Epoch: 191, Training RMSE: 0.802835, Test RMSE 0.939277\n",
      "Epoch: 192, Training RMSE: 0.802122, Test RMSE 0.939225\n",
      "Epoch: 193, Training RMSE: 0.801412, Test RMSE 0.939182\n",
      "Epoch: 194, Training RMSE: 0.800709, Test RMSE 0.939143\n",
      "Epoch: 195, Training RMSE: 0.800008, Test RMSE 0.939103\n",
      "Epoch: 196, Training RMSE: 0.799313, Test RMSE 0.939065\n",
      "Epoch: 197, Training RMSE: 0.798625, Test RMSE 0.939032\n",
      "Epoch: 198, Training RMSE: 0.797937, Test RMSE 0.938988\n",
      "Epoch: 199, Training RMSE: 0.797262, Test RMSE 0.938948\n",
      "Epoch: 200, Training RMSE: 0.796598, Test RMSE 0.938916\n",
      "Epoch: 201, Training RMSE: 0.795934, Test RMSE 0.938872\n",
      "Epoch: 202, Training RMSE: 0.795278, Test RMSE 0.938831\n",
      "Epoch: 203, Training RMSE: 0.794630, Test RMSE 0.938796\n",
      "Epoch: 204, Training RMSE: 0.793981, Test RMSE 0.938756\n",
      "Epoch: 205, Training RMSE: 0.793338, Test RMSE 0.938723\n",
      "Epoch: 206, Training RMSE: 0.792701, Test RMSE 0.938698\n",
      "Epoch: 207, Training RMSE: 0.792068, Test RMSE 0.938679\n",
      "Epoch: 208, Training RMSE: 0.791443, Test RMSE 0.938650\n",
      "Epoch: 209, Training RMSE: 0.790829, Test RMSE 0.938619\n",
      "Epoch: 210, Training RMSE: 0.790220, Test RMSE 0.938586\n",
      "Epoch: 211, Training RMSE: 0.789624, Test RMSE 0.938558\n",
      "Epoch: 212, Training RMSE: 0.789032, Test RMSE 0.938539\n",
      "Epoch: 213, Training RMSE: 0.788450, Test RMSE 0.938526\n",
      "Epoch: 214, Training RMSE: 0.787872, Test RMSE 0.938522\n",
      "Epoch: 215, Training RMSE: 0.787295, Test RMSE 0.938520\n",
      "Epoch: 216, Training RMSE: 0.786725, Test RMSE 0.938521\n",
      "Epoch: 217, Training RMSE: 0.786158, Test RMSE 0.938526\n",
      "Epoch: 218, Training RMSE: 0.785600, Test RMSE 0.938535\n",
      "Epoch: 219, Training RMSE: 0.785045, Test RMSE 0.938549\n",
      "Epoch: 220, Training RMSE: 0.784491, Test RMSE 0.938557\n",
      "Epoch: 221, Training RMSE: 0.783939, Test RMSE 0.938560\n",
      "Epoch: 222, Training RMSE: 0.783392, Test RMSE 0.938563\n",
      "Epoch: 223, Training RMSE: 0.782852, Test RMSE 0.938564\n",
      "Epoch: 224, Training RMSE: 0.782314, Test RMSE 0.938564\n",
      "Epoch: 225, Training RMSE: 0.781776, Test RMSE 0.938578\n",
      "Epoch: 226, Training RMSE: 0.781244, Test RMSE 0.938584\n",
      "Epoch: 227, Training RMSE: 0.780718, Test RMSE 0.938591\n",
      "Epoch: 228, Training RMSE: 0.780198, Test RMSE 0.938604\n",
      "Epoch: 229, Training RMSE: 0.779680, Test RMSE 0.938612\n",
      "Epoch: 230, Training RMSE: 0.779171, Test RMSE 0.938620\n",
      "Epoch: 231, Training RMSE: 0.778662, Test RMSE 0.938634\n",
      "Epoch: 232, Training RMSE: 0.778155, Test RMSE 0.938645\n",
      "Epoch: 233, Training RMSE: 0.777654, Test RMSE 0.938653\n",
      "Epoch: 234, Training RMSE: 0.777157, Test RMSE 0.938669\n",
      "Epoch: 235, Training RMSE: 0.776667, Test RMSE 0.938693\n",
      "Epoch: 236, Training RMSE: 0.776184, Test RMSE 0.938718\n",
      "Epoch: 237, Training RMSE: 0.775703, Test RMSE 0.938743\n",
      "Epoch: 238, Training RMSE: 0.775228, Test RMSE 0.938780\n",
      "Epoch: 239, Training RMSE: 0.774764, Test RMSE 0.938815\n",
      "Epoch: 240, Training RMSE: 0.774304, Test RMSE 0.938844\n",
      "Epoch: 241, Training RMSE: 0.773850, Test RMSE 0.938862\n",
      "Epoch: 242, Training RMSE: 0.773395, Test RMSE 0.938879\n",
      "Epoch: 243, Training RMSE: 0.772945, Test RMSE 0.938895\n",
      "Epoch: 244, Training RMSE: 0.772497, Test RMSE 0.938913\n",
      "Epoch: 245, Training RMSE: 0.772058, Test RMSE 0.938929\n",
      "Epoch: 246, Training RMSE: 0.771626, Test RMSE 0.938946\n",
      "Epoch: 247, Training RMSE: 0.771201, Test RMSE 0.938967\n",
      "Epoch: 248, Training RMSE: 0.770779, Test RMSE 0.938983\n",
      "Epoch: 249, Training RMSE: 0.770359, Test RMSE 0.939004\n",
      "Epoch: 250, Training RMSE: 0.769941, Test RMSE 0.939027\n",
      "Epoch: 251, Training RMSE: 0.769529, Test RMSE 0.939049\n",
      "Epoch: 252, Training RMSE: 0.769111, Test RMSE 0.939075\n",
      "Epoch: 253, Training RMSE: 0.768694, Test RMSE 0.939103\n",
      "Epoch: 254, Training RMSE: 0.768282, Test RMSE 0.939128\n",
      "Epoch: 255, Training RMSE: 0.767866, Test RMSE 0.939151\n",
      "Epoch: 256, Training RMSE: 0.767453, Test RMSE 0.939178\n",
      "Epoch: 257, Training RMSE: 0.767045, Test RMSE 0.939206\n",
      "Epoch: 258, Training RMSE: 0.766641, Test RMSE 0.939223\n",
      "Epoch: 259, Training RMSE: 0.766244, Test RMSE 0.939246\n",
      "Epoch: 260, Training RMSE: 0.765847, Test RMSE 0.939277\n",
      "Epoch: 261, Training RMSE: 0.765456, Test RMSE 0.939306\n",
      "Epoch: 262, Training RMSE: 0.765069, Test RMSE 0.939336\n",
      "Epoch: 263, Training RMSE: 0.764684, Test RMSE 0.939362\n",
      "Epoch: 264, Training RMSE: 0.764301, Test RMSE 0.939398\n",
      "Epoch: 265, Training RMSE: 0.763923, Test RMSE 0.939448\n",
      "Epoch: 266, Training RMSE: 0.763547, Test RMSE 0.939500\n",
      "Epoch: 267, Training RMSE: 0.763174, Test RMSE 0.939549\n",
      "Epoch: 268, Training RMSE: 0.762803, Test RMSE 0.939585\n",
      "Epoch: 269, Training RMSE: 0.762443, Test RMSE 0.939616\n",
      "Epoch: 270, Training RMSE: 0.762079, Test RMSE 0.939641\n",
      "Epoch: 271, Training RMSE: 0.761722, Test RMSE 0.939679\n",
      "Epoch: 272, Training RMSE: 0.761367, Test RMSE 0.939722\n",
      "Epoch: 273, Training RMSE: 0.761018, Test RMSE 0.939773\n",
      "Epoch: 274, Training RMSE: 0.760670, Test RMSE 0.939816\n",
      "Epoch: 275, Training RMSE: 0.760319, Test RMSE 0.939860\n",
      "Epoch: 276, Training RMSE: 0.759969, Test RMSE 0.939902\n",
      "Epoch: 277, Training RMSE: 0.759628, Test RMSE 0.939943\n",
      "Epoch: 278, Training RMSE: 0.759295, Test RMSE 0.939988\n",
      "Epoch: 279, Training RMSE: 0.758966, Test RMSE 0.940023\n",
      "Epoch: 280, Training RMSE: 0.758640, Test RMSE 0.940071\n",
      "Epoch: 281, Training RMSE: 0.758314, Test RMSE 0.940111\n",
      "Epoch: 282, Training RMSE: 0.757994, Test RMSE 0.940166\n",
      "Epoch: 283, Training RMSE: 0.757679, Test RMSE 0.940214\n",
      "Epoch: 284, Training RMSE: 0.757361, Test RMSE 0.940255\n",
      "Epoch: 285, Training RMSE: 0.757044, Test RMSE 0.940289\n",
      "Epoch: 286, Training RMSE: 0.756734, Test RMSE 0.940327\n",
      "Epoch: 287, Training RMSE: 0.756427, Test RMSE 0.940370\n",
      "Epoch: 288, Training RMSE: 0.756121, Test RMSE 0.940404\n",
      "Epoch: 289, Training RMSE: 0.755817, Test RMSE 0.940438\n",
      "Epoch: 290, Training RMSE: 0.755512, Test RMSE 0.940466\n",
      "Epoch: 291, Training RMSE: 0.755208, Test RMSE 0.940493\n",
      "Epoch: 292, Training RMSE: 0.754905, Test RMSE 0.940521\n",
      "Epoch: 293, Training RMSE: 0.754604, Test RMSE 0.940560\n",
      "Epoch: 294, Training RMSE: 0.754306, Test RMSE 0.940607\n",
      "Epoch: 295, Training RMSE: 0.754014, Test RMSE 0.940662\n",
      "Epoch: 296, Training RMSE: 0.753719, Test RMSE 0.940728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 297, Training RMSE: 0.753427, Test RMSE 0.940788\n",
      "Epoch: 298, Training RMSE: 0.753138, Test RMSE 0.940850\n",
      "Epoch: 299, Training RMSE: 0.752851, Test RMSE 0.940899\n",
      "Epoch: 300, Training RMSE: 0.752567, Test RMSE 0.940943\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch_pmf.fit(torch_train, torch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_acc,recall_acc:(0.048510638297872555, 0.025632800405455444)\n"
     ]
    }
   ],
   "source": [
    "plt.plot(range(torch_pmf.maxepoch), torch_pmf.rmse_train, marker='o', label='Training Data')\n",
    "plt.plot(range(torch_pmf.maxepoch), torch_pmf.rmse_test, marker='v', label='Test Data')\n",
    "plt.title('The MovieLens Dataset Learning Curve')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(\"precision_acc,recall_acc:\" + str(torch_pmf.topK(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sir_log_weights(x, target, proposal):\n",
    "    return target.log_prob(x) -  proposal.log_prob(x)\n",
    "\n",
    "def sir_correlated_dynamics_pmf(z, target, proposal, n_steps, N, alpha, partition, train_vec, test_vec):\n",
    "    z_sp = []\n",
    "    batch_size, z_dim = z.shape[0], z.shape[1]\n",
    "    rmse_train = []\n",
    "    rmse_test = []\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        z_sp.append(z)\n",
    "        z_copy = z.unsqueeze(1).repeat(1, N, 1)\n",
    "        ind = torch.randint(0, N, (batch_size,)).tolist()\n",
    "        W = proposal.sample([batch_size, N])\n",
    "        U = proposal.sample([batch_size]).unsqueeze(1).repeat(1, N, 1)\n",
    "        #print(W.shape, U.shape, z_copy.shape)\n",
    "        X = torch.zeros((batch_size, N, z_dim), dtype = z.dtype).to(z.device)\n",
    "        X =  (alpha**2)*z_copy + alpha*((1- alpha**2)**0.5)*U + W*((1- alpha**2)**0.5)\n",
    "        X[np.arange(batch_size), ind, :] = z\n",
    "        X_view = X.view(-1, z_dim)\n",
    "\n",
    "        log_weight = compute_sir_log_weights(X_view, target, proposal)\n",
    "        log_weight = log_weight.view(batch_size, N)\n",
    "        max_logs = torch.max(log_weight, dim = 1)[0][:, None]\n",
    "        log_weight = log_weight - max_logs\n",
    "        weight = torch.exp(log_weight)\n",
    "        sum_weight = torch.sum(weight, dim = 1)\n",
    "        weight = weight/sum_weight[:, None]        \n",
    "\n",
    "        weight[weight != weight] = 0.\n",
    "        weight[weight.sum(1) == 0.] = 1.\n",
    "\n",
    "        indices = torch.multinomial(weight, 1).squeeze().tolist()\n",
    "\n",
    "        z = X[np.arange(batch_size), indices, :]\n",
    "        z = z.data\n",
    "        \n",
    "        target.w_User = z[:partition]\n",
    "        target.w_Item = z[partition:]\n",
    "        \n",
    "        ratings = train_vec[:, 2]\n",
    "        concat_shape = target.w_User[np.array(train_vec[:, 0], dtype='int32'), :].shape[0]\n",
    "        w_big = torch.cat((target.w_User[np.array(train_vec[:, 0], dtype='int32'), :], \n",
    "                           target.w_Item[np.array(train_vec[:, 1], dtype='int32'), :]))\n",
    "        obj = target.loss(w_big, ratings, concat_shape, False).item()\n",
    "        obj += 0.5 * self._lambda * (torch.norm(target.w_User) ** 2 + torch.norm(target.w_Item) ** 2)\n",
    "\n",
    "        rmse_train.append((obj / pairs_train) ** 0.5)\n",
    "        \n",
    "        ratings = test_vec[:, 2]\n",
    "        concat_shape = target.w_User[np.array(test_vec[:, 0], dtype='int32'), :].shape[0]\n",
    "        w_big = torch.cat((target.w_User[np.array(test_vec[:, 0], dtype='int32'), :], \n",
    "                           target.w_Item[np.array(test_vec[:, 1], dtype='int32'), :]))\n",
    "\n",
    "        loss = target.loss(w_big, ratings, concat_shape, use_regularize = False).item()\n",
    "        rmse_test.append((loss / pairs_test)** 0.5)\n",
    "\n",
    "        print('Iter: %d, Training RMSE: %f, Test RMSE %f' % (_,\n",
    "                                                             rmse_train[-1], \n",
    "                                                             rmse_test[-1]))\n",
    "        \n",
    "    z_sp.append(z)\n",
    "    return z_sp, rmse_train, rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "scale_proposal = 0.1\n",
    "proposal = init_independent_normal(scale_proposal, torch_pmf.num_feat, device)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "c = 1.0\n",
    "N = 10\n",
    "n_steps = 20\n",
    "alpha = (1 - c/torch_pmf.num_feat)**0.5\n",
    "\n",
    "target = torch_pmf\n",
    "ratings = torch_train[:, 2]\n",
    "concat_shape = torch_pmf.w_User.shape[0]\n",
    "num_items = torch_pmf.w_Item.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(944, 1683)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_shape, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (944) must match the size of tensor b (25326) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-a5a7ae9554ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproposal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconcat_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_items\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m history, rmse_train, rmse_test = sir_correlated_dynamics_pmf(start, \n\u001b[0m\u001b[1;32m      9\u001b[0m                                                              \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                              \u001b[0mproposal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-44fa89f3c3dc>\u001b[0m in \u001b[0;36msir_correlated_dynamics_pmf\u001b[0;34m(z, target, proposal, n_steps, N, alpha, partition, train_vec, test_vec)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mX_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mlog_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_sir_log_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_view\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mlog_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmax_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-44fa89f3c3dc>\u001b[0m in \u001b[0;36mcompute_sir_log_weights\u001b[0;34m(x, target, proposal)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_sir_log_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m  \u001b[0mproposal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msir_correlated_dynamics_pmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mz_sp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-4e799f6dd94f>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, params, ratings, partition, use_regularize, log_prob)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_regularize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         pred_out = torch.sum(torch.multiply(params[:partition],\n\u001b[0m\u001b[1;32m     25\u001b[0m                                             params[partition:]),\n\u001b[1;32m     26\u001b[0m                                      axis=1)  # mean_inv subtracted # np.multiply对应位置元素相乘\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (944) must match the size of tensor b (25326) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "target.log_prob = partial(target.loss, \n",
    "                          ratings = ratings, \n",
    "                          partition = concat_shape, \n",
    "                          use_regularize = True, \n",
    "                          log_prob = True)\n",
    "\n",
    "start = proposal.sample([concat_shape + num_items])\n",
    "history, rmse_train, rmse_test = sir_correlated_dynamics_pmf(start, \n",
    "                                                             target,\n",
    "                                                             proposal, \n",
    "                                                             n_steps, \n",
    "                                                             N,\n",
    "                                                             alpha,\n",
    "                                                             concat_shape, \n",
    "                                                             torch_train, \n",
    "                                                             torch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

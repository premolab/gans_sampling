{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitfad426787c4d466e858501df55a3f5b8",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "\n",
    "api_path = os.path.join(cwd, '..', 'tools', 'sampling_utils')\n",
    "sys.path.append(api_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from sir_ais_sampling import (run_experiments_gaussians,\n",
    "                              run_experiments_2_gaussians,\n",
    "                              sir_correlated_dynamics,\n",
    "                              sir_independent_dynamics)\n",
    "\n",
    "from ebm_sampling import (citerais_mala_dynamics,\n",
    "                          citerais_ula_dynamics, \n",
    "                          i_ais_z_dynamics,\n",
    "                          i_ais_v_dynamics,\n",
    "                          i_ais_b_dynamics)\n",
    "\n",
    "from distributions import (Target, \n",
    "                           Gaussian_mixture, \n",
    "                           IndependentNormal,\n",
    "                           init_independent_normal)\n",
    "\n",
    "from torch.distributions import (MultivariateNormal, \n",
    "                                 Normal, \n",
    "                                 Independent, \n",
    "                                 Uniform)\n",
    "\n",
    "from metrics import Evolution\n",
    "from general_utils import DotDict, send_file_to_remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebm_sampling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def i_ais_v_dynamics(z, target, n_steps, grad_step, eps_scale, N, betas, rho):\n",
    "#     z_sp = [z.clone().detach()]\n",
    "#     batch_size, z_dim = z.shape[0], z.shape[1]\n",
    "#     device = z.device\n",
    "\n",
    "#     mh_transition = MH_Transition(z_dim, device)\n",
    "#     acceptence_rate = [] #0.0\n",
    "\n",
    "#     betas = np.array(betas)\n",
    "#     betas_diff = torch.FloatTensor(betas[:-1] - betas[1:]) #n-1\n",
    "    \n",
    "#     scale_proposal = 1.0\n",
    "#     proposal = init_independent_normal(scale_proposal, z_dim, device)\n",
    "\n",
    "#     z_last = z\n",
    "#     for step in tqdm(range(n_steps)):\n",
    "#         if step == 0:\n",
    "#             N_ = N\n",
    "#         else:\n",
    "#             N_ = N - 1\n",
    "\n",
    "#         z = z_last.unsqueeze(1).repeat(1, N_, 1)\n",
    "\n",
    "#         z = rho*z + ((1 - rho**2)**0.5) * proposal.sample([batch_size, N_])\n",
    "        \n",
    "#         z_batch = torch.transpose(z, 0, 1).reshape((batch_size*N_, z_dim)).detach().clone()\n",
    "#         z_batch.requires_grad_(True)\n",
    "\n",
    "#         # E = E.unsqueeze(1).repeat(1, N)\n",
    "#         # grad = grad.unsqueeze(1).repeat(1, N, 1)\n",
    "#         # grad = torch.transpose(grad, 0, 1).reshape((batch_size*N, z_dim)).data #detach().clone()\n",
    "\n",
    "#         E, grad = grad_energy(z_batch, target)\n",
    "#         E = E.reshape(N_, batch_size).T\n",
    "\n",
    "#         z_backward = z_batch\n",
    "#         z_backward.requires_grad_(True)\n",
    "#         energy_backward = torch.zeros(batch_size, N_, len(betas)-1) # n-1\n",
    "#         energy_backward[..., len(betas)-2] = E #.detach().clone()\n",
    "        \n",
    "#         E = E.T.reshape(-1)\n",
    "\n",
    "#         accept = torch.zeros([len(betas)-2, N - 1])\n",
    "#         for i, beta in enumerate(betas[::-1][1:-1]):  #betas[0] = 1, betas[n-1] = 0, betas[::-1][1:-1] - increasing, lenghts is n-2\n",
    "#             j = len(betas) - i - 3\n",
    "#             eps = eps_scale * proposal.sample([batch_size*N_])\n",
    "\n",
    "#             z_backward_new = z_backward - beta * grad_step * grad + eps\n",
    "#             z_backward, E, grad, mask = mh_transition.do_transition_step(z_backward, \n",
    "#                                                                          z_backward_new, \n",
    "#                                                                          E, \n",
    "#                                                                          grad, \n",
    "#                                                                          grad_step, \n",
    "#                                                                          eps_scale, \n",
    "#                                                                          target, \n",
    "#                                                                          beta=beta)\n",
    "#             if step > 0:\n",
    "#                 mask = mask.float().reshape(z.shape[:-1][::-1]).T.mean(0)\n",
    "#                 accept[i] = mask #.sum()\n",
    "#             E_ = E.reshape(z.shape[:-1][::-1]).T\n",
    "#             energy_backward[..., j] = E_.detach().clone()\n",
    "#         if step > 0:\n",
    "#             acceptence_rate.append(accept)\n",
    "\n",
    "#         z_backward = torch.transpose(z_backward.reshape(list(z.shape[:-1][::-1]) + [z.shape[-1]]), 0, 1)\n",
    "#         #grad = torch.transpose(grad.reshape(list(z.shape[:-1][::-1]) + [z.shape[-1]]), 0, 1)\n",
    "#         #E = E.reshape(z.shape[:-1][::-1]).T\n",
    "\n",
    "#         if step > 0:\n",
    "#             z = torch.cat([z, z_last.unsqueeze(1)], dim=1)\n",
    "#             z_backward = torch.cat([z_backward, z_backward_last.unsqueeze(1)], dim=1)\n",
    "#             energy_backward = torch.cat([energy_backward, energy_backward_last.unsqueeze(1)], dim=1)\n",
    "#             #grad = torch.cat([grad, grad_last.unsqueeze(1)], dim=1)\n",
    "\n",
    "#         #E = energy_backward[..., 0]\n",
    "\n",
    "#         F_backward = (betas_diff[None, None, :] * energy_backward).sum(-1)\n",
    "#         log_weights = -F_backward\n",
    "\n",
    "#         max_logs = torch.max(log_weights, dim = 1)[0]\n",
    "#         log_weights = log_weights - max_logs[:, None]\n",
    "#         sum_weights = torch.logsumexp(log_weights, dim = 1)\n",
    "#         log_weights = log_weights- sum_weights[:, None]\n",
    "#         weights = log_weights.exp()\n",
    "#         weights[weights != weights] = 0.\n",
    "#         weights[weights.sum(1) == 0.] = 1.\n",
    "\n",
    "#         indices = torch.multinomial(weights, 1).squeeze().tolist()\n",
    "\n",
    "#         z_last = z[np.arange(batch_size), indices, :]\n",
    "#         z_last = z_last.data\n",
    "#         z_last.requires_grad_(True)\n",
    "\n",
    "#         z_backward_last = z_backward[np.arange(batch_size), indices, :]\n",
    "#         z_sp.append(z_backward_last.detach().clone())\n",
    "\n",
    "#         #grad_last = grad[np.arange(batch_size), indices, :].data\n",
    "#         #grad_last.requires_grad_(True)\n",
    "\n",
    "#         energy_backward_last = energy_backward[np.arange(batch_size), indices, :]\n",
    "\n",
    "#     #acceptence_rate = acceptence_rate/(batch_size*N)/len(betas[::-1][1:-1])/n_steps\n",
    "#     #acceptence_rate = acceptence_rate/(n_steps - 1)\n",
    "#     acceptence_rate = torch.stack(acceptence_rate, 0)\n",
    "#     return z_sp, acceptence_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_arr = [2] + [30*(i + 1) for i in range(9)]  \n",
    "\n",
    "loc_target = 3.0\n",
    "loc_proposal = 0.0\n",
    "\n",
    "var_proposal = 2.0\n",
    "var_target = 1.0\n",
    "scale_proposal = var_proposal ** 0.5\n",
    "scale_target = var_target ** 0.5\n",
    "num_points_in_chain = 100\n",
    "strategy_mean = 'chain'\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "batch_size = 100\n",
    "n_steps = 300"
   ]
  },
  {
   "source": [
    "dim = 2\n",
    "\n",
    "target = init_independent_normal(scale_target, dim, \n",
    "                                       device, loc_target)\n",
    "proposal = init_independent_normal(scale_proposal, dim, \n",
    "                                    device, loc_proposal)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "\n",
    "T = 50\n",
    "betas = list(np.linspace(1., 0., T))\n",
    "rhos = np.linspace(1, .7, T)\n",
    "\n",
    "#betas = list(np.linspace(1., 0.85, 50)) + list(np.linspace(0.85, 0., 30))\n",
    "\n",
    "grad_step = 0.1\n",
    "eps_scale = (2*grad_step)**0.5\n",
    "\n",
    "method_params = {'n_steps': n_steps, \n",
    "                 'N': N, \n",
    "                 'grad_step': grad_step, \n",
    "                 'eps_scale': eps_scale, \n",
    "                 'betas': betas,\n",
    "                 'rho': 1.0,\n",
    "                 'rhos': rhos}\n",
    "random_seed = 42\n",
    "method = 'citerais_mala' #'i_ais_v' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = proposal.sample([batch_size, len(betas)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 300/300 [00:27<00:00, 11.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# history, acceptence = i_ais_v_dynamics(start, \n",
    "#                                         target.log_prob,\n",
    "#                                         method_params['n_steps'], \n",
    "#                                         method_params['grad_step'], \n",
    "#                                         method_params['eps_scale'],\n",
    "#                                         method_params['N'], \n",
    "#                                         method_params['betas'],\n",
    "#                                         method_params['rho'])\n",
    "                                            \n",
    "history, acceptence = citerais_ula_dynamics(start, \n",
    "                                                target.log_prob,\n",
    "                                                method_params['n_steps'], \n",
    "                                                method_params['grad_step'], \n",
    "                                                method_params['eps_scale'],\n",
    "                                                method_params['N'], \n",
    "                                                method_params['betas'],\n",
    "                                                method_params['rhos']\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(78)+1, acceptence[120].mean(1))\n",
    "# plt.savefig('../../accept_rate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acceptence[-1].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_history = history[max(1, len(history) - num_points_in_chain - 1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_history = history[-num_points_in_chain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(last_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.stack(last_history, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.39393940567970276\n"
     ]
    }
   ],
   "source": [
    "print((result[1:] != result[:-1]).float().min(-1)[0].mean(0)[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_np = result.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over chain\n",
    "result_var = np.var(result_np, axis = 0, ddof=1).mean(axis = 0).mean()\n",
    "result_mean = np.mean(result_np, axis = 0).mean(axis = 0).mean()\n",
    "\n",
    "# over batch\n",
    "result_var = np.var(result_np, axis = 1, ddof=1).mean(axis = 0).mean()\n",
    "result_mean = np.mean(result_np, axis = 1).mean(axis = 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(100, 100, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "result_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1.612894, 0.5227851)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "result_mean, result_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 2/1000 [00:00<01:01, 16.27it/s]------------------\n",
      "mode = proposal\n",
      "dim = 2\n",
      "  5%|▌         | 50/1000 [00:02<00:45, 20.99it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-722-aff4074a5f9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m results_citerais_ula = run_experiments_gaussians(dim_arr,  \n\u001b[0m\u001b[1;32m     21\u001b[0m                                           \u001b[0mscale_proposal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                           \u001b[0mscale_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gans_sampling/notebooks/../tools/sampling_utils/sir_ais_sampling.py\u001b[0m in \u001b[0;36mrun_experiments_gaussians\u001b[0;34m(dim_arr, scale_proposal, scale_target, loc_target, num_points_in_chain, strategy_mean, device, batch_size, method_params, random_seed, loc_proposal, mode_init, method, print_results)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'i_ais_v'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m          history, acceptence = ebm_sampling.i_ais_v_dynamics(start, \n\u001b[0m\u001b[1;32m    190\u001b[0m                                                 \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                                                 \u001b[0mmethod_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gans_sampling/notebooks/../tools/sampling_utils/ebm_sampling.py\u001b[0m in \u001b[0;36mi_ais_v_dynamics\u001b[0;34m(z, target, n_steps, grad_step, eps_scale, N, betas, rho)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;31m#z_backward_new = (1. - grad_step) * z_backward - beta * grad_step * grad + eps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mz_backward_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_backward\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_step\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             z_backward, E, grad, mask = mala_transition.do_transition_step(z_backward, \n\u001b[0m\u001b[1;32m    472\u001b[0m                                                                          \u001b[0mz_backward_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                                                                          \u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gans_sampling/notebooks/../tools/sampling_utils/ebm_sampling.py\u001b[0m in \u001b[0;36mdo_transition_step\u001b[0;34m(self, z, z_new, energy, grad, grad_step, sigma, target, beta)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_transition_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0macc_log_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_log_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mgenerate_uniform_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gans_sampling/notebooks/../tools/sampling_utils/ebm_sampling.py\u001b[0m in \u001b[0;36mcompute_log_probs\u001b[0;34m(self, z, z_new, energy, grad, grad_step, sigma, target, beta)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_log_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0menergy_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mlog_transition_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_langevin_transition_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mlog_transition_backward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_langevin_transition_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gans_sampling/notebooks/../tools/sampling_utils/ebm_sampling.py\u001b[0m in \u001b[0;36mgrad_energy\u001b[0;34m(point, target, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "T = 50\n",
    "betas = np.linspace(1., 0., T)\n",
    "\n",
    "grad_step = 0.5\n",
    "eps_scale = (2*grad_step)**0.5\n",
    "\n",
    "method_params = {'n_steps': n_steps, \n",
    "                 'N': N, \n",
    "                 'grad_step': grad_step, \n",
    "                 'eps_scale': eps_scale, \n",
    "                 'betas': betas,\n",
    "                 'rho': 1.0}\n",
    "random_seed = 42\n",
    "method = 'i_ais_v' \n",
    "mode_init = 'proposal'\n",
    "print_results = True\n",
    "\n",
    "results_citerais_ula = run_experiments_gaussians(dim_arr,  \n",
    "                                          scale_proposal, \n",
    "                                          scale_target, \n",
    "                                          loc_target,\n",
    "                                          num_points_in_chain, \n",
    "                                          strategy_mean,\n",
    "                                          device,\n",
    "                                          batch_size,\n",
    "                                          method_params,\n",
    "                                          random_seed = random_seed,\n",
    "                                          loc_proposal = loc_proposal,\n",
    "                                          mode_init = mode_init,\n",
    "                                          method = method,\n",
    "                                          print_results = print_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'chain'"
      ]
     },
     "metadata": {},
     "execution_count": 674
    }
   ],
   "source": [
    "strategy_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import ot\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch, torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "from easydict import EasyDict as edict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterative_sir.toy_examples_utils.toy_examples_utils import prepare_swissroll_data\n",
    "from iterative_sir.toy_examples_utils.gan_fc_models import (\n",
    "    Generator_fc, \n",
    "    Discriminator_fc,\n",
    "    )\n",
    "from iterative_sir.sampling_utils.visualization import (\n",
    "                           sample_fake_data,\n",
    "                           plot_discriminator_2d,\n",
    "                           mh_sampling_plot_2d,\n",
    "                           langevin_sampling_plot_2d,\n",
    "                           mala_sampling_plot_2d,\n",
    "                           plot_chain_metrics)\n",
    "from iterative_sir.sampling_utils.ebm_sampling import (\n",
    "                          langevin_sampling,\n",
    "                          mala_dynamics, \n",
    "                          mala_sampling,\n",
    "                          gan_energy,\n",
    "                          IndependentNormal)\n",
    "from iterative_sir.sampling_utils.adaptive_mc import ex2_mcmc_mala\n",
    "from iterative_sir.toy_examples_utils.params_swissroll_wasserstein import (random_seed,\n",
    "                                          train_dataset_size,\n",
    "                                          n_dim,\n",
    "                                          n_layers_d,\n",
    "                                          n_layers_g,\n",
    "                                          n_hid_d,\n",
    "                                          n_hid_g,\n",
    "                                          n_out,\n",
    "                                          device)\n",
    "\n",
    "from iterative_sir.sampling_utils.metrics import Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "figpath = Path('../figs')\n",
    "\n",
    "assert figpath.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "X_train = prepare_swissroll_data(train_dataset_size)\n",
    "X_train_std = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator_fc(n_dim=n_dim, \n",
    "                 n_layers=n_layers_g,\n",
    "                 n_hid=n_hid_g,\n",
    "                 n_out=n_out,\n",
    "                 non_linear=nn.ReLU(),\n",
    "                 device=device)\n",
    "D = Discriminator_fc(n_in=n_dim, \n",
    "                     n_layers=n_layers_d,\n",
    "                     n_hid=n_hid_d,\n",
    "                     non_linear=nn.ReLU(),\n",
    "                     device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = sorted(Path('../models/models_swissroll/wasserstein').glob('*_generator.pth'))[0]\n",
    "disc_path = sorted(Path('../models/models_swissroll/wasserstein').glob('*_discriminator.pth'))[0]\n",
    "\n",
    "G.load_state_dict(torch.load(gen_path, map_location=device))\n",
    "D.load_state_dict(torch.load(disc_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = G.n_dim\n",
    "loc = torch.zeros(n_dim).to(G.device)\n",
    "scale = torch.ones(n_dim).to(G.device)\n",
    "normal = Normal(loc, scale)\n",
    "normalize_to_0_1 = True \n",
    "log_prob = True\n",
    "\n",
    "proposal = IndependentNormal(\n",
    "    dim=n_dim,\n",
    "    device=device,\n",
    "    loc=loc,\n",
    "    scale=scale)\n",
    "\n",
    "target = partial(gan_energy, \n",
    "                     generator = G, \n",
    "                     discriminator = D, \n",
    "                     proposal = proposal,\n",
    "                     normalize_to_0_1 = normalize_to_0_1,\n",
    "                     log_prob = log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evols = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 #25 #5000\n",
    "n_steps = 800\n",
    "every = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sample = X_train[np.random.choice(np.arange(X_train.shape[0]), 1000)]\n",
    "\n",
    "grad_step = 1e-1 #3e-3\n",
    "eps_scale = (grad_step * 2) ** 0.5\n",
    "\n",
    "z_last_np, zs = langevin_sampling(target,\n",
    "                               proposal,  \n",
    "                               batch_size=batch_size,\n",
    "                               n = batch_size,\n",
    "                               grad_step = grad_step,\n",
    "                               eps_scale = eps_scale,\n",
    "                               n_steps = n_steps)\n",
    "\n",
    "n_chunks = len(zs[0]) // every\n",
    "zs = zs[0, -n_chunks * every:].reshape((n_chunks, batch_size, -1, zs.shape[-1]))\n",
    "zs_gen = zs.reshape(batch_size, n_chunks, -1, zs.shape[-1])\n",
    "\n",
    "Xs_gen = G(torch.FloatTensor(zs_gen).to(device)).detach().cpu().numpy()\n",
    "#Xs_gen = scaler.inverse_transform(Xs_gen.reshape(-1, Xs_gen.shape[-1])).reshape(Xs_gen.shape)\n",
    "\n",
    "evol = defaultdict(list)\n",
    "for X_gen in Xs_gen:\n",
    "    evolution = Evolution(target_sample,\n",
    "                      target_log_prob=target)\n",
    "    for chunk in X_gen:\n",
    "        evolution.invoke(torch.FloatTensor(chunk))\n",
    "    evol_ = evolution.as_dict()\n",
    "    for k, v in evol_.items():\n",
    "        evol[k].append(v)\n",
    "\n",
    "for k, v in evol.items():\n",
    "    evol[k] = (np.mean(np.array(v), 0), np.std(np.array(v), 0, ddof=1) / np.sqrt(batch_size))\n",
    "evols['ULA'] = evol"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
